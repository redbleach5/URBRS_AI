"""
Models router - API –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ LLM
–í—ã–±–æ—Ä, –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π, –ø–æ–ª–∏—Ç–∏–∫–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
"""

from fastapi import APIRouter, HTTPException, Request
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any, List

from backend.core.logger import get_logger
from backend.core.types import CostTier, RoutingPolicy, ProviderInfo

logger = get_logger(__name__)

router = APIRouter()


# ============ Routing Policy Models ============

class RoutingPolicyRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"""
    prefer_local: bool = Field(True, description="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama)")
    require_private: bool = Field(False, description="–°—Ç—Ä–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å")
    max_cost_tier: int = Field(4, ge=1, le=4, description="–ú–∞–∫—Å. —Å—Ç–æ–∏–º–æ—Å—Ç—å: 1=FREE, 2=CHEAP, 3=STANDARD, 4=PREMIUM")
    prefer_cheap: bool = Field(False, description="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å –¥–µ—à—ë–≤—ã–µ –º–æ–¥–µ–ª–∏")
    prefer_quality: bool = Field(True, description="–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å–∫–æ—Ä–æ—Å—Ç–∏")
    min_quality: float = Field(0.5, ge=0, le=1, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞")
    allowed_providers: Optional[List[str]] = Field(None, description="–†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã")
    blocked_providers: Optional[List[str]] = Field(None, description="–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã")


class RoutingPolicyResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"""
    success: bool
    policy: Dict[str, Any]
    presets: Dict[str, Dict[str, Any]]


class ProviderInfoResponse(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–µ"""
    name: str
    is_local: bool
    is_private: bool
    cost_tier: int
    cost_tier_name: str
    enabled: bool
    description: str


class ProvidersInfoResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    success: bool
    providers: List[ProviderInfoResponse]
    default_provider: str


class ModelInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    provider: str  # ollama, openai, anthropic
    size: Optional[str] = None  # 1b, 7b, 13b, 70b –∏ —Ç.–¥.
    capabilities: List[str] = []  # chat, code, vision, reasoning
    quality_score: float = 0.7  # 0.0 - 1.0
    speed_score: float = 0.5  # —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ
    is_available: bool = True
    is_recommended: bool = False
    description: Optional[str] = None


class ModelsResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å–æ —Å–ø–∏—Å–∫–æ–º –º–æ–¥–µ–ª–µ–π"""
    success: bool
    models: List[ModelInfo]
    current_model: Optional[str] = None
    auto_select_enabled: bool = True
    resource_level: str = "low"


class ModelSelectRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏"""
    model: Optional[str] = None  # None = –∞–≤—Ç–æ–≤—ã–±–æ—Ä
    provider: Optional[str] = "ollama"
    auto_select: bool = True  # –í–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–≤—ã–±–æ—Ä


class ModelSelectResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –Ω–∞ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏"""
    success: bool
    selected_model: str
    provider: str
    auto_selected: bool
    reason: str


# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
MODEL_INFO_DB: Dict[str, Dict[str, Any]] = {
    # –ú–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ (1-3B) - –±—ã—Å—Ç—Ä—ã–µ
    "gemma3:1b": {
        "size": "1b",
        "capabilities": ["chat", "code"],
        "quality_score": 0.55,
        "speed_score": 0.95,
        "description": "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á"
    },
    "gemma2:2b": {
        "size": "2b",
        "capabilities": ["chat", "code"],
        "quality_score": 0.6,
        "speed_score": 0.9,
        "description": "–ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞"
    },
    "phi3:mini": {
        "size": "3.8b",
        "capabilities": ["chat", "code", "reasoning"],
        "quality_score": 0.65,
        "speed_score": 0.85,
        "description": "–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Microsoft"
    },
    "tinyllama": {
        "size": "1.1b",
        "capabilities": ["chat"],
        "quality_score": 0.5,
        "speed_score": 0.98,
        "description": "–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á"
    },
    
    # –°—Ä–µ–¥–Ω–∏–µ –º–æ–¥–µ–ª–∏ (7-8B) - –±–∞–ª–∞–Ω—Å
    "llama3:8b": {
        "size": "8b",
        "capabilities": ["chat", "code", "reasoning"],
        "quality_score": 0.8,
        "speed_score": 0.7,
        "description": "–û—Ç–ª–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á"
    },
    "llama3.1:8b": {
        "size": "8b",
        "capabilities": ["chat", "code", "reasoning"],
        "quality_score": 0.82,
        "speed_score": 0.7,
        "description": "–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Llama 3"
    },
    "mistral:7b": {
        "size": "7b",
        "capabilities": ["chat", "code", "reasoning"],
        "quality_score": 0.78,
        "speed_score": 0.75,
        "description": "–ë—ã—Å—Ç—Ä–∞—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è"
    },
    "gemma2:9b": {
        "size": "9b",
        "capabilities": ["chat", "code"],
        "quality_score": 0.75,
        "speed_score": 0.7,
        "description": "Google Gemma 2"
    },
    "codellama:7b": {
        "size": "7b",
        "capabilities": ["code"],
        "quality_score": 0.8,
        "speed_score": 0.75,
        "description": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∫–æ–¥–µ"
    },
    "deepseek-coder:6.7b": {
        "size": "6.7b",
        "capabilities": ["code"],
        "quality_score": 0.82,
        "speed_score": 0.75,
        "description": "–û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"
    },
    "qwen2.5-coder:7b": {
        "size": "7b",
        "capabilities": ["code", "chat"],
        "quality_score": 0.85,
        "speed_score": 0.72,
        "description": "–¢–æ–ø –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞ –æ—Ç Alibaba"
    },
    
    # –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (13-14B)
    "llama3:13b": {
        "size": "13b",
        "capabilities": ["chat", "code", "reasoning"],
        "quality_score": 0.88,
        "speed_score": 0.5,
        "description": "–ú–æ—â–Ω–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å"
    },
    "codellama:13b": {
        "size": "13b",
        "capabilities": ["code"],
        "quality_score": 0.88,
        "speed_score": 0.5,
        "description": "–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞"
    },
    
    # –û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (30B+)
    "llama3:70b": {
        "size": "70b",
        "capabilities": ["chat", "code", "reasoning", "analysis"],
        "quality_score": 0.95,
        "speed_score": 0.2,
        "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
    },
    "qwen2.5:72b": {
        "size": "72b",
        "capabilities": ["chat", "code", "reasoning", "analysis"],
        "quality_score": 0.96,
        "speed_score": 0.18,
        "description": "–¢–æ–ø–æ–≤–∞—è –º–æ–¥–µ–ª—å Alibaba"
    },
    "deepseek-coder:33b": {
        "size": "33b",
        "capabilities": ["code"],
        "quality_score": 0.92,
        "speed_score": 0.3,
        "description": "–ü—Ä–µ–º–∏—É–º –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞"
    },
    
    # Vision –º–æ–¥–µ–ª–∏
    "llava:7b": {
        "size": "7b",
        "capabilities": ["chat", "vision"],
        "quality_score": 0.75,
        "speed_score": 0.6,
        "description": "–ú–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
    },
    "llava:13b": {
        "size": "13b",
        "capabilities": ["chat", "vision"],
        "quality_score": 0.82,
        "speed_score": 0.4,
        "description": "–£–ª—É—á—à–µ–Ω–Ω–∞—è vision –º–æ–¥–µ–ª—å"
    },
}


def _extract_model_size(model_name: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"""
    import re
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–º–µ—Ä–∞: 1b, 7b, 13b, 70b, 1.5b –∏ —Ç.–¥.
    patterns = [
        r'(\d+\.?\d*)[bB]',  # 7b, 70b, 1.5b
        r':(\d+)[bB]',  # :7b
    ]
    
    for pattern in patterns:
        match = re.search(pattern, model_name)
        if match:
            return f"{match.group(1)}b"
    
    return None


def _get_model_info(model_name: str, provider: str = "ollama") -> ModelInfo:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è (—É–±–∏—Ä–∞–µ–º :latest –∏ —Ç.–ø.)
    model_name.split(":")[0] if ":" in model_name else model_name
    full_name = model_name.lower()
    
    # –ò—â–µ–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    info = None
    for key, data in MODEL_INFO_DB.items():
        if key.lower() in full_name or full_name in key.lower():
            info = data
            break
    
    if info:
        return ModelInfo(
            name=model_name,
            provider=provider,
            size=info.get("size"),
            capabilities=info.get("capabilities", ["chat"]),
            quality_score=info.get("quality_score", 0.7),
            speed_score=info.get("speed_score", 0.5),
            is_available=True,
            description=info.get("description")
        )
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤ –±–∞–∑–µ, –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
    size = _extract_model_size(model_name)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ —Ä–∞–∑–º–µ—Ä—É
    quality_score = 0.7
    speed_score = 0.5
    capabilities = ["chat"]
    
    if size:
        size_num = float(size.replace("b", ""))
        if size_num <= 3:
            quality_score = 0.55
            speed_score = 0.9
        elif size_num <= 8:
            quality_score = 0.75
            speed_score = 0.7
        elif size_num <= 15:
            quality_score = 0.85
            speed_score = 0.5
        elif size_num <= 35:
            quality_score = 0.9
            speed_score = 0.3
        else:
            quality_score = 0.95
            speed_score = 0.2
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º capabilities –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
    name_lower = model_name.lower()
    if "code" in name_lower or "coder" in name_lower:
        capabilities = ["code", "chat"]
    if "vision" in name_lower or "llava" in name_lower:
        capabilities.append("vision")
    
    return ModelInfo(
        name=model_name,
        provider=provider,
        size=size,
        capabilities=capabilities,
        quality_score=quality_score,
        speed_score=speed_score,
        is_available=True,
        description=f"–ú–æ–¥–µ–ª—å {model_name}"
    )


@router.get("/models", response_model=ModelsResponse)
async def get_available_models(request: Request):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        engine = request.app.state.engine
        if not engine or not engine.llm_manager:
            return ModelsResponse(
                success=False,
                models=[],
                current_model=None,
                auto_select_enabled=True,
                resource_level="unknown"
            )
        
        models_list: List[ModelInfo] = []
        current_model = None
        resource_level = "low"
        
        # –ü–æ–ª—É—á–∞–µ–º Ollama –º–æ–¥–µ–ª–∏
        ollama_provider = engine.llm_manager.providers.get("ollama")
        if ollama_provider:
            try:
                available = await ollama_provider.list_models()
                current_model = ollama_provider.default_model
                
                for model_name in available:
                    info = _get_model_info(model_name, "ollama")
                    models_list.append(info)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º resource_level
                if any(m.size and float(m.size.replace("b", "")) >= 30 for m in models_list if m.size):
                    resource_level = "high"
                elif any(m.size and float(m.size.replace("b", "")) >= 13 for m in models_list if m.size):
                    resource_level = "medium"
                elif any(m.size and float(m.size.replace("b", "")) >= 7 for m in models_list if m.size):
                    resource_level = "low"
                else:
                    resource_level = "minimal"
                    
            except Exception as e:
                logger.warning(f"Failed to get Ollama models: {e}")
        
        # TODO: –î–æ–±–∞–≤–∏—Ç—å –º–æ–¥–µ–ª–∏ OpenAI/Anthropic –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
        openai_provider = engine.llm_manager.providers.get("openai")
        if openai_provider:
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ OpenAI
            openai_models = ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]
            for model_name in openai_models:
                models_list.append(ModelInfo(
                    name=model_name,
                    provider="openai",
                    capabilities=["chat", "code", "reasoning"],
                    quality_score=0.95 if "gpt-4" in model_name else 0.8,
                    speed_score=0.6,
                    is_available=True,
                    description=f"OpenAI {model_name}"
                ))
        
        anthropic_provider = engine.llm_manager.providers.get("anthropic")
        if anthropic_provider:
            anthropic_models = ["claude-3-5-sonnet-20241022", "claude-3-haiku-20240307"]
            for model_name in anthropic_models:
                models_list.append(ModelInfo(
                    name=model_name,
                    provider="anthropic",
                    capabilities=["chat", "code", "reasoning", "analysis"],
                    quality_score=0.97 if "sonnet" in model_name else 0.85,
                    speed_score=0.5 if "sonnet" in model_name else 0.8,
                    is_available=True,
                    description=f"Anthropic {model_name}"
                ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        models_list.sort(key=lambda m: m.quality_score, reverse=True)
        
        # –ü–æ–º–µ—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ª—É—á—à–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        recommended_sizes = set()
        for model in models_list:
            if model.size and model.size not in recommended_sizes:
                model.is_recommended = True
                recommended_sizes.add(model.size)
                if len(recommended_sizes) >= 3:
                    break
        
        return ModelsResponse(
            success=True,
            models=models_list,
            current_model=current_model,
            auto_select_enabled=True,
            resource_level=resource_level
        )
        
    except Exception as e:
        logger.error(f"Error getting models: {e}")
        return ModelsResponse(
            success=False,
            models=[],
            current_model=None,
            auto_select_enabled=True,
            resource_level="unknown"
        )


@router.post("/models/select", response_model=ModelSelectResponse)
async def select_model(req: ModelSelectRequest, request: Request):
    """–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    try:
        engine = request.app.state.engine
        if not engine or not engine.llm_manager:
            raise HTTPException(status_code=503, detail="Engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        ollama_provider = engine.llm_manager.providers.get("ollama")
        if not ollama_provider:
            raise HTTPException(status_code=503, detail="Ollama provider –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
        
        if req.auto_select or not req.model:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä - –∏—Å–ø–æ–ª—å–∑—É–µ–º ResourceAwareSelector
            # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            return ModelSelectResponse(
                success=True,
                selected_model=ollama_provider.default_model,
                provider="ollama",
                auto_selected=True,
                reason="–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤"
            )
        
        # –†—É—á–Ω–æ–π –≤—ã–±–æ—Ä
        available = await ollama_provider.list_models()
        
        if req.model not in available:
            raise HTTPException(
                status_code=400, 
                detail=f"–ú–æ–¥–µ–ª—å '{req.model}' –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {', '.join(available[:10])}"
            )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ default
        ollama_provider.default_model = req.model
        
        logger.info(f"Model manually selected: {req.model}")
        
        return ModelSelectResponse(
            success=True,
            selected_model=req.model,
            provider=req.provider or "ollama",
            auto_selected=False,
            reason=f"–ú–æ–¥–µ–ª—å '{req.model}' –≤—ã–±—Ä–∞–Ω–∞ –≤—Ä—É—á–Ω—É—é"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error selecting model: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/models/recommend")
async def recommend_model(
    request: Request,
    task_type: Optional[str] = None,  # code, chat, research, analysis
    complexity: Optional[str] = None,  # low, medium, high
    speed_priority: bool = False
):
    """–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏"""
    try:
        engine = request.app.state.engine
        if not engine or not engine.llm_manager:
            raise HTTPException(status_code=503, detail="Engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        ollama_provider = engine.llm_manager.providers.get("ollama")
        if not ollama_provider:
            raise HTTPException(status_code=503, detail="–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤")
        
        available = await ollama_provider.list_models()
        if not available:
            raise HTTPException(status_code=503, detail="–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        models_info = [_get_model_info(m, "ollama") for m in available]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ capabilities
        if task_type == "code":
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –º–æ–¥–µ–ª–∏ —Å capability "code"
            models_info.sort(key=lambda m: ("code" in m.capabilities, m.quality_score), reverse=True)
        elif task_type == "vision":
            models_info = [m for m in models_info if "vision" in m.capabilities]
        
        # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å
        if speed_priority:
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏
            models_info.sort(key=lambda m: m.speed_score, reverse=True)
        elif complexity == "high":
            # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á - –∫–∞—á–µ—Å—Ç–≤–æ –≤–∞–∂–Ω–µ–µ
            models_info.sort(key=lambda m: m.quality_score, reverse=True)
        elif complexity == "low":
            # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö - –±–∞–ª–∞–Ω—Å (–∫–∞—á–µ—Å—Ç–≤–æ * —Å–∫–æ—Ä–æ—Å—Ç—å)
            models_info.sort(key=lambda m: m.quality_score * m.speed_score, reverse=True)
        
        if not models_info:
            # Fallback
            return {
                "success": True,
                "recommended": available[0],
                "alternatives": available[1:3] if len(available) > 1 else [],
                "reason": "Fallback: –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å"
            }
        
        recommended = models_info[0]
        alternatives = [m.name for m in models_info[1:4]]
        
        return {
            "success": True,
            "recommended": recommended.name,
            "recommended_info": recommended.dict(),
            "alternatives": alternatives,
            "reason": f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –¥–ª—è {task_type or '–æ–±—â–∏—Ö –∑–∞–¥–∞—á'} "
                     f"(–∫–∞—á–µ—Å—Ç–≤–æ: {recommended.quality_score:.0%}, —Å–∫–æ—Ä–æ—Å—Ç—å: {recommended.speed_score:.0%})"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error recommending model: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============ Routing Policy Endpoints ============

@router.get("/models/routing-policy", response_model=RoutingPolicyResponse)
async def get_routing_policy(request: Request):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–æ–ª–∏—Ç–∏–∫—É –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"""
    try:
        engine = request.app.state.engine
        if not engine:
            raise HTTPException(status_code=503, detail="Engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–∏—Ç–∏–∫—É –∏–∑ UnifiedModelRouter
        from backend.core.unified_model_router import get_unified_router
        
        try:
            router_instance = get_unified_router()
            policy = router_instance.DEFAULT_POLICY
        except ValueError:
            # Router –µ—â—ë –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É
            policy = RoutingPolicy()
        
        # –ì–æ—Ç–æ–≤–∏–º preset –ø–æ–ª–∏—Ç–∏–∫–∏
        presets = {
            "privacy_first": RoutingPolicy.privacy_first().to_dict(),
            "cost_first": RoutingPolicy.cost_first().to_dict(),
            "quality_first": RoutingPolicy.quality_first().to_dict(),
            "balanced": RoutingPolicy.balanced().to_dict(),
        }
        
        return RoutingPolicyResponse(
            success=True,
            policy=policy.to_dict(),
            presets=presets
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting routing policy: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/models/routing-policy", response_model=RoutingPolicyResponse)
async def update_routing_policy(policy_req: RoutingPolicyRequest, request: Request):
    """–û–±–Ω–æ–≤–∏—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏"""
    try:
        engine = request.app.state.engine
        if not engine:
            raise HTTPException(status_code=503, detail="Engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –ø–æ–ª–∏—Ç–∏–∫—É
        new_policy = RoutingPolicy(
            prefer_local=policy_req.prefer_local,
            require_private=policy_req.require_private,
            max_cost_tier=CostTier(policy_req.max_cost_tier),
            prefer_cheap=policy_req.prefer_cheap,
            prefer_quality=policy_req.prefer_quality,
            min_quality=policy_req.min_quality,
            allowed_providers=policy_req.allowed_providers,
            blocked_providers=policy_req.blocked_providers,
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–∏—Ç–∏–∫—É –≤ UnifiedModelRouter
        from backend.core.unified_model_router import get_unified_router
        
        try:
            router_instance = get_unified_router()
            router_instance.DEFAULT_POLICY = new_policy
            logger.info(f"Routing policy updated: prefer_local={new_policy.prefer_local}, "
                       f"require_private={new_policy.require_private}, "
                       f"max_cost={new_policy.max_cost_tier.name}")
        except ValueError:
            # Router –µ—â—ë –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
            logger.warning("Router not initialized, policy will be applied on next init")
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ config.yaml –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        # –≠—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ —á–µ—Ä–µ–∑ update_config endpoint
        
        presets = {
            "privacy_first": RoutingPolicy.privacy_first().to_dict(),
            "cost_first": RoutingPolicy.cost_first().to_dict(),
            "quality_first": RoutingPolicy.quality_first().to_dict(),
            "balanced": RoutingPolicy.balanced().to_dict(),
        }
        
        return RoutingPolicyResponse(
            success=True,
            policy=new_policy.to_dict(),
            presets=presets
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating routing policy: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/models/providers-info", response_model=ProvidersInfoResponse)
async def get_providers_info(request: Request):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞—Ö (local, private, cost)"""
    try:
        engine = request.app.state.engine
        if not engine:
            raise HTTPException(status_code=503, detail="Engine –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        from backend.core.unified_model_router import PROVIDER_INFO
        from backend.config import get_config
        
        config = get_config()
        providers_config = config.llm.providers
        default_provider = config.llm.default_provider
        
        providers_list = []
        
        # –û–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        descriptions = {
            "ollama": "ü¶ô –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ ‚Äî –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –ø—Ä–∏–≤–∞—Ç–Ω–æ, –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞",
            "openai": "ü§ñ OpenAI GPT ‚Äî –æ–±–ª–∞—á–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è API –∫–ª—é—á",
            "anthropic": "üß† Anthropic Claude ‚Äî –æ–±–ª–∞—á–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è API –∫–ª—é—á",
        }
        
        for provider_name, provider_info in PROVIDER_INFO.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á—ë–Ω –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä
            provider_cfg = getattr(providers_config, provider_name, None)
            enabled = provider_cfg.enabled if provider_cfg else False
            
            providers_list.append(ProviderInfoResponse(
                name=provider_name,
                is_local=provider_info.is_local,
                is_private=provider_info.is_private,
                cost_tier=provider_info.cost_tier.value,
                cost_tier_name=provider_info.cost_tier.name,
                enabled=enabled,
                description=descriptions.get(provider_name, f"–ü—Ä–æ–≤–∞–π–¥–µ—Ä {provider_name}")
            ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ, –ø–æ—Ç–æ–º –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        providers_list.sort(key=lambda p: (not p.is_local, p.cost_tier))
        
        return ProvidersInfoResponse(
            success=True,
            providers=providers_list,
            default_provider=default_provider
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting providers info: {e}")
        raise HTTPException(status_code=500, detail=str(e))

